from sentence_transformers import SentenceTransformer
import time
import numpy as np

class EmbeddingService:
    def __init__(self, model_name="BAAI/bge-base-en-v1.5"):
        self.model = None
        self.model_name = model_name
        self.load_time = 0
        self.inference_times = []
    
    def load(self):
        """åŠ è½½å¹¶é¢„çƒ­æ¨¡å‹"""
        if self.model is None:
            start = time.time()
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½ {self.model_name} æ¨¡å‹...")
            
            self.model = SentenceTransformer(
                self.model_name,
                device="cpu",  # ä½¿ç”¨CPU
                # quantize=True  # å¦‚éœ€é‡åŒ–å–æ¶ˆæ³¨é‡Š(éœ€å®‰è£…onnxruntime)
            )
            
            # é¢„çƒ­æ¨¡å‹
            self.model.encode(["warmup text"], batch_size=1)
            self.load_time = time.time() - start
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {self.load_time:.1f}ç§’")
    
    def embed(self, texts, normalize=True):
        """ç”Ÿæˆembeddingå¹¶è®°å½•æ€§èƒ½"""
        if not isinstance(texts, list):
            texts = [texts]
            
        start_time = time.time()
        embeddings = self.model.encode(texts, normalize_embeddings=normalize)
        elapsed = time.time() - start_time
        self.inference_times.append(elapsed)
        return embeddings
    
    def get_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            "model": self.model_name,
            "load_time": self.load_time,
            "avg_inference": np.mean(self.inference_times) if self.inference_times else 0,
            "total_requests": len(self.inference_times)
        }

# ============ ä½¿ç”¨ç¤ºä¾‹ ============ 
if __name__ == "__main__":
    # åˆå§‹åŒ–æœåŠ¡
    service = EmbeddingService()
    service.load()  # é¦–æ¬¡åŠ è½½
    
    # æµ‹è¯•æ–‡æœ¬
    texts = [
        "Qdrant is a vector search engine",
        "BGE is a powerful embedding model",
        "This is a test sentence"
    ]
    
    # ç”Ÿæˆembedding
    print("\nğŸ” æµ‹è¯•embeddingç”Ÿæˆ...")
    start_inference = time.time()
    embeddings = service.embed(texts, normalize=True)
    
    # æ‰“å°ç»“æœ
    print(f"Embeddingç»´åº¦: {embeddings.shape[1]}")  # è¾“å‡º 768
    print(f"æœ¬æ¬¡æ¨ç†è€—æ—¶: {time.time()-start_inference:.3f}ç§’")
    print(f"ç¤ºä¾‹embeddingå€¼: {embeddings[0][:5]}...")  # å‰5ä¸ªå€¼
    
    # æ€§èƒ½ç»Ÿè®¡
    stats = service.get_stats()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯: å¹³å‡è€—æ—¶ {stats['avg_inference']:.3f}ç§’/æ¬¡, æ€»è¯·æ±‚æ•° {stats['total_requests']}")

