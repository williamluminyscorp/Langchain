import json
import uuid
import hashlib
import time
import numpy as np
from fastapi import FastAPI, HTTPException
import uvicorn
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams
from qdrant_client.models import PointStruct, Filter, FieldCondition, MatchValue
from sentence_transformers import SentenceTransformer
import logging
from pydantic import BaseModel
from typing import Optional
from openai import OpenAI
from typing import Optional,List
import os
from dotenv import load_dotenv
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EmbeddingService:
    def __init__(self, model_name="BAAI/bge-base-en-v1.5"):
        self.model = None
        self.model_name = model_name
        self.load_time = 0
        self.inference_times = []
        self.qdrant_client = QdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),  # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤å€¼localhost
            port=int(os.getenv("QDRANT_PORT", 6333))     # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤å€¼6333
        )
        self.openai_client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY")          # ä»ç¯å¢ƒå˜é‡è¯»å–
        )
    def load(self):
        """åŠ è½½å¹¶é¢„çƒ­æ¨¡å‹"""
        if self.model is None:
            start = time.time()
            logger.info(f"ğŸ”„ æ­£åœ¨åŠ è½½ {self.model_name} æ¨¡å‹...")
            
            self.model = SentenceTransformer(
                self.model_name,
                device="cpu",  # ä½¿ç”¨CPU
                # quantize=True  # å¦‚éœ€é‡åŒ–å–æ¶ˆæ³¨é‡Š(éœ€å®‰è£…onnxruntime)
            )
            
            # é¢„çƒ­æ¨¡å‹
            self.model.encode(["warmup text"], batch_size=1)
            self.load_time = time.time() - start
            logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {self.load_time:.1f}ç§’")
    
    def embed(self, texts, normalize=True):
        """ç”Ÿæˆembeddingå¹¶è®°å½•æ€§èƒ½"""
        if not isinstance(texts, list):
            texts = [texts]
            
        start_time = time.time()
        embeddings = self.model.encode(texts, normalize_embeddings=normalize)
        elapsed = time.time() - start_time
        self.inference_times.append(elapsed)
        return embeddings
    
    def get_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            "model": self.model_name,
            "load_time": self.load_time,
            "avg_inference": np.mean(self.inference_times) if self.inference_times else 0,
            "total_requests": len(self.inference_times)
        }
    def configure_openai(self, api_key: str):
        """é…ç½®OpenAIå®¢æˆ·ç«¯"""
        self.openai_client = OpenAI(api_key=api_key)
        logger.info("âœ… OpenAIå®¢æˆ·ç«¯å·²é…ç½®")
    
    def search_similar_texts(self, query_vec: List[float], collection_name: str, top_k: int = 3):
        try:
            results = self.qdrant_client.search(
                collection_name=collection_name,
                query_vector=query_vec,
                limit=top_k,
                with_payload=True,
                with_vectors=False,
            )
            return [
                {
                    "id": hit.id,
                    "score": hit.score,
                    "text": hit.payload["text"],
                    "payload": hit.payload  # å®Œæ•´payload
                }
                for hit in results
            ]
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")

    def store_embeddings(self, texts, collection_name="pdf_chunks"):
        """å­˜å‚¨embeddingåˆ°Qdrant"""
        if not isinstance(texts, list):
            texts = [texts]
            
        embeddings = self.embed(texts)
        content_hashes = [hashlib.md5(text.encode('utf-8')).hexdigest() for text in texts]
        
        # Get or create collection
        try:
            collections = self.qdrant_client.get_collections()
            if collection_name not in [col.name for col in collections.collections]:
                self.qdrant_client.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.COSINE)
                )
                # åˆ›å»ºé›†åˆåç«‹å³å»ºç´¢å¼•
                self.qdrant_client.create_payload_index(
                    collection_name=collection_name,
                    field_name="content_hash",
                    field_schema="keyword",
                    wait=True
                )
                logger.info(f"Created new collection '{collection_name}' with index")
            
            # Prepare points for upsert
            points = []
            for text, embedding, content_hash in zip(texts, embeddings, content_hashes):
                # Check for duplicates
                existing_points = self.qdrant_client.scroll(
                    collection_name=collection_name,
                    scroll_filter=Filter(
                        must=[FieldCondition(key="content_hash", match=MatchValue(value=content_hash))]
                    ),
                    limit=1,
                    with_payload=["content_hash"],
                    with_vectors=False
                )
                
                if not existing_points[0]:  # Only insert if not exists
                    points.append(PointStruct(
                        id=str(uuid.uuid4()),
                        vector=embedding.tolist(),
                        payload={"text": text, "content_hash": content_hash}
                    ))
            
            if points:
                response = self.qdrant_client.upsert(
                    collection_name=collection_name,
                    points=points,
                    wait=True
                )
                logger.info(f"Inserted {len(points)} embeddings successfully! Status: {response.status}")
                return True
            else:
                logger.info("All texts already exist in the collection")
                return False
                
        except Exception as e:
            logger.error(f"Storage operation failed: {e}")
            return False
        
    def generate_answer(self, query: str, context: str, model: str = "gpt-3.5-turbo", temperature: float = 0.3):
        """ä½¿ç”¨GPTç”Ÿæˆå›ç­”"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¬å¸çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹å…¬å¸æ‰‹å†Œå†…å®¹å›ç­”é—®é¢˜ï¼š
        
        ã€ç›¸å…³æ‰‹å†Œå†…å®¹ã€‘
        {context}

        ã€ç”¨æˆ·é—®é¢˜ã€‘
        {query}

        è¦æ±‚ï¼š
        1. å›ç­”å¿…é¡»å®Œå…¨åŸºäºæä¾›çš„æ‰‹å†Œå†…å®¹
        2. å¦‚æœå†…å®¹ä¸­æ²¡æœ‰æ˜ç¡®ç­”æ¡ˆï¼Œè¯·å›ç­”"æ ¹æ®æ‰‹å†Œå†…å®¹ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        3. ä¿æŒå›ç­”ä¸“ä¸šã€ç®€æ´
        """

        response = self.openai_client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
        )
        return response.choices[0].message.content
    

class CreateCollectionRequest(BaseModel):
    collection_name: str
    vector_size: Optional[int] = None  # å¯é€‰ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨æ¨¡å‹é»˜è®¤ç»´åº¦
    distance: Optional[str] = "COSINE"  # é»˜è®¤ä¸ºCOSINEç›¸ä¼¼åº¦

# æ·»åŠ è¿™ä¸ªç±»å®šä¹‰
class QueryRequest(BaseModel):
    question: str
    collection_name: str = "pdf_chunks"
    top_k: int = 3
    openai_model: str = "gpt-3.5-turbo"
    temperature: float = 0.3

# Initialize FastAPI application
app = FastAPI()
service = EmbeddingService()
service.load()

@app.post("/embed")
async def embed(texts: list[str]):
    """API endpoint to generate embeddings"""
    return {"embeddings": service.embed(texts).tolist()}

@app.post("/store")
async def store(texts: list[str], collection: str = "pdf_chunks"):
    """API endpoint to store embeddings"""
    success = service.store_embeddings(texts, collection_name=collection)
    return {"status": "success" if success else "failed"}

@app.post("/ask")
async def ask_question(request: QueryRequest):
    """
    å®Œæ•´é—®ç­”æµç¨‹:
    1. é—®é¢˜å‘é‡åŒ–
    2. æœç´¢ç›¸ä¼¼å†…å®¹
    3. è°ƒç”¨OpenAIç”Ÿæˆå›ç­”
    è¿”å›: å›ç­” + æœç´¢ç»“æœè¯¦æƒ…
    """
    try:
        # 1. ç”Ÿæˆé—®é¢˜å‘é‡
        query_vec = service.embed(request.question).tolist()[0]
        
        # 2. æœç´¢ç›¸ä¼¼å†…å®¹ï¼ˆè¿”å›å®Œæ•´ç»“æœï¼‰
        search_results = service.search_similar_texts(
            query_vec=query_vec,
            collection_name=request.collection_name,
            top_k=request.top_k
        )
        
        # æå–æ–‡æœ¬ç”¨äºç”Ÿæˆå›ç­”
        context = "\n\n".join([hit["text"] for hit in search_results])
        
        # 3. ç”Ÿæˆå›ç­”
        answer = service.generate_answer(
            query=request.question,
            context=context,
            model=request.openai_model,
            temperature=request.temperature
        )
        
        return {
            "status": "success",
            "answer": answer,
            "search_results": search_results,  # åŒ…å«IDã€å¾—åˆ†ã€æ–‡æœ¬ç­‰
            "used_model": request.openai_model
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/create_collection")
async def create_collection(request: CreateCollectionRequest):
    """åˆ›å»ºæ–°çš„å‘é‡é›†åˆ"""
    try:
        # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
        collections = service.qdrant_client.get_collections()
        if request.collection_name in [col.name for col in collections.collections]:
            return {"status": "failed", "reason": "Collection already exists"}
        
        # ç¡®å®šå‘é‡ç»´åº¦
        vector_size = request.vector_size if request.vector_size else service.embed(["sample"]).shape[1]
        
        # åˆ›å»ºæ–°é›†åˆ
        service.qdrant_client.create_collection(
            collection_name=request.collection_name,
            vectors_config=VectorParams(
                size=vector_size,
                distance=Distance[request.distance] if request.distance else Distance.COSINE
            )
        )
        
        # åˆ›å»ºå†…å®¹å“ˆå¸Œç´¢å¼•
        service.qdrant_client.create_payload_index(
            collection_name=request.collection_name,
            field_name="content_hash",
            field_schema="keyword",
            wait=True
        )
        
        return {
            "status": "success",
            "collection_name": request.collection_name,
            "vector_size": vector_size,
            "distance": request.distance or "COSINE"
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/stats")
async def stats():
    """API endpoint to get service statistics"""
    return service.get_stats()

if __name__ == "__main__":
    logger.info("æ­£åœ¨å¯åŠ¨æœåŠ¡...")
    
    # Example usage
    if False:  # Set to True to run test cases
        test_texts = [
            "Qdrant is a vector search engine",
            "BGE is a powerful embedding model",
            "This is a test sentence"
        ]
        
        # Test embedding generation
        logger.info("\nğŸ” æµ‹è¯•embeddingç”Ÿæˆ...")
        start_inference = time.time()
        embeddings = service.embed(test_texts)
        
        logger.info(f"Embeddingç»´åº¦: {embeddings.shape[1]}")
        logger.info(f"æœ¬æ¬¡æ¨ç†è€—æ—¶: {time.time()-start_inference:.3f}ç§’")
        logger.info(f"ç¤ºä¾‹embeddingå€¼: {embeddings[0][:5]}...")
        
        # Test storage
        service.store_embeddings(test_texts)
        
        # Print stats
        stats = service.get_stats()
        logger.info(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯: å¹³å‡è€—æ—¶ {stats['avg_inference']:.3f}ç§’/æ¬¡, æ€»è¯·æ±‚æ•° {stats['total_requests']}")
    
    # Start FastAPI server
    uvicorn.run(app, host="0.0.0.0", port=8000)